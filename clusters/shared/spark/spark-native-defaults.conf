#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Cluster config
spark.master=spark://coordinator:7077
spark.ui.reverseProxy=true

# Enable application event logging, app status metrics
spark.eventLog.enabled=true
spark.eventLog.dir=/opt/spark/events
spark.metrics.appStatusSource.enabled=true

# Must have dependencies compatible with our hms
spark.sql.hive.metastore.jars=maven
spark.sql.hive.metastore.version=3.1.3

# Global session catalog support
spark.hadoop.hive.metastore.uris=thrift://a74b6e20f132e4eac840a591d5caf0ba-c3bc190f0aee9cb5.elb.us-east-1.amazonaws.com:9083
spark.sql.catalogImplementation=hive

# Wrap the default catalog with support for Iceberg and non-Iceberg tables
spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type=hive

# Add iceberg catalog, only supports Iceberg tables
spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type=hive
spark.sql.catalog.iceberg.uri=thrift://a74b6e20f132e4eac840a591d5caf0ba-c3bc190f0aee9cb5.elb.us-east-1.amazonaws.com:9083
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider

# Use Iceberg extension module to Spark to add new SQL commands
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# Gluten specific config
spark.gluten.sql.debug=false
spark.sql.adaptive.enabled=true
spark.gluten.sql.columnar.forceShuffledHashJoin=true
spark.plugins=org.apache.gluten.GlutenPlugin
spark.memory.offHeap.enabled=true
spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager
spark.hadoop.fs.s3a.endpoint=https://s3.us-east-1.amazonaws.com
spark.hadoop.fs.s3a.connection.ssl.enabled=true
spark.hadoop.fs.s3a.use.instance.credentials=true
spark.cleaner.periodicGC.interval=10s
spark.gluten.sql.columnar.backend.lib=velox
spark.gluten.sql.columnar.maxBatchSize=4096
spark.executorEnv.LD_PRELOAD=/usr/lib64/libjemalloc.so.2
spark.gluten.sql.columnar.coalesce.batches=true
spark.gluten.sql.columnar.shuffle.codec=zstd
spark.gluten.memory.overAcquiredMemoryRatio=0

# TPCDS Q72
spark.gluten.sql.columnar.joinOptimizationLevel=18
spark.gluten.sql.columnar.physicalJoinOptimizeEnable=true
spark.gluten.sql.columnar.physicalJoinOptimizationLevel=18
spark.gluten.sql.columnar.logicalJoinOptimizeEnable=true

# Perf Opt
spark.sql.files.maxPartitionBytes=2g 
spark.gluten.sql.columnar.maxBatchSize=4096
# smallest number without Spill triggered
spark.default.parallelism=56
# smallest number without Spill triggered
spark.sql.shuffle.partitions=56
# 3x of executor.cores
spark.gluten.sql.columnar.backend.velox.IOThreads=21
spark.gluten.sql.columnar.backend.velox.prefetchRowGroups=1 
spark.gluten.sql.columnar.backend.velox.cacheEnabled=false 
spark.gluten.sql.columnar.backend.velox.memCacheSize=134217728 
spark.gluten.sql.columnar.backend.velox.ssdCacheSize=0
spark.gluten.sql.columnar.backend.velox.maxCoalescedDistanceBytes=1048576 
spark.gluten.sql.columnar.backend.velox.loadQuantum=268435456 
spark.gluten.sql.columnar.backend.velox.maxCoalescedBytes=67108864 
spark.gluten.sql.columnar.backend.velox.SplitPreloadPerDriver=0
spark.gluten.memory.overAcquiredMemoryRatio=0


# Spark config
spark.sql.autoBroadcastJoinThreshold=10m
spark.sql.optimizer.dynamicPartitionPruning.enabled=true
spark.cleaner.periodicGC.interval=10s
spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold=0
spark.sql.optimizer.runtime.bloomFilter.enabled=true
spark.io.compression.codec=lz4
spark.driver.memory=12g
spark.driver.memoryOverhead=5g
